{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOSfbZJBIUkm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math as math\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import statsmodels.api as sm\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df=pd.read_csv('/content/drive/MyDrive/nga-east_rotd50_5pct_flatfile_public_20141118.csv')"
      ],
      "metadata": {
        "id": "vuXdsGGBIak4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = df.drop(df[df['ClstD (km)'] == -999 ].index)\n",
        "# df = df.drop(df[df['Preferred VS30 (m/sec)'] == -999 ].index)\n",
        "df = df.drop_duplicates()\n",
        "# df = df.drop(df[df['Quality Flag Based on Ground Motions Residuals'] == -999 ].index)\n",
        "# df = df.drop(df[df['Depth Used (km)'] == -999 ].index)\n",
        "# df = df.drop(df[df['Hypocenter Depth (km)'] == -999 ].index)\n",
        "df = df.drop(df[df['Joyner-Boore Dist. (km)'] == -999 ].index)\n",
        "df = df.drop(df[df['Joyner-Boore Dist. (km)'] > 1500 ].index)\n",
        "df = df.drop(df[df['PGA-H RotDnn (g)'] == -999 ].index)\n",
        "# df = df.drop(df[df['Station Latitude (deg)'] == -999 ].index)\n",
        "# df = df.drop(df[df['Station Longitude (deg)'] == -999 ].index)\n",
        "# df = df.drop(df[df['Earthquake Magnitude'] < 0 ].index)\n",
        "df = df.drop(df[df['HypD (km)'] <= 0 ].index)"
      ],
      "metadata": {
        "id": "6wmSKav-IcKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rows, cols = df.shape\n",
        "\n",
        "print(\"Number of rows:\", rows)\n",
        "print(\"Number of columns:\", cols)"
      ],
      "metadata": {
        "id": "8-sosbkPIfFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index()"
      ],
      "metadata": {
        "id": "j1Qtm5SpIdpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense, Lambda\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.losses import mse\n"
      ],
      "metadata": {
        "id": "OMWGgrlEIhts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define VAE architecture\n",
        "\n",
        "\n",
        "# def leaky_relu(x, alpha=0.2):\n",
        "#     return tf.maximum(alpha * x, x)\n",
        "\n",
        "def create_vae(input_dim, latent_dim, intermediate_dim,intermediate_dim_1,intermediate_dim_2,intermediate_dim_3):\n",
        "\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "\n",
        "    x = Dense(intermediate_dim, activation='relu')(input_layer)\n",
        "    x = Dense(intermediate_dim_1, activation='relu')(x)\n",
        "    x = Dense(intermediate_dim_2, activation='relu')(x)\n",
        "    x = Dense(intermediate_dim_3, activation='relu')(x)\n",
        "    # x = Dense(intermediate_dim_4, activation='sigmoid')(x)\n",
        "    # x = Dense(intermediate_dim_5, activation='sigmoid')(x)\n",
        "    # x = Dense(intermediate_dim_6, activation='sigmoid')(x)\n",
        "\n",
        "\n",
        "\n",
        "    z_mean = Dense(latent_dim)(x)\n",
        "    z_log_var = Dense(latent_dim)(x)\n",
        "\n",
        "\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.0, stddev=1.0)\n",
        "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n",
        "    z = Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "\n",
        "    # decoder_h = Dense(intermediate_dim_6, activation='sigmoid')\n",
        "    # decoder_h = Dense(intermediate_dim_5, activation='sigmoid')\n",
        "    # decoder_h = Dense(intermediate_dim_4, activation='sigmoid')\n",
        "    decoder_h = Dense(intermediate_dim_3, activation='relu')\n",
        "    decoder_h = Dense(intermediate_dim_2, activation='relu')\n",
        "    decoder_h = Dense(intermediate_dim_1, activation='relu')\n",
        "    decoder_h = Dense(intermediate_dim, activation='relu')\n",
        "    decoder_mean = Dense(input_dim, name='decoder_output')  # Output layer with name 'decoder_output'\n",
        "\n",
        "    h_decoded = decoder_h(z)\n",
        "    x_decoded_mean = decoder_mean(h_decoded)\n",
        "\n",
        "\n",
        "    vae = Model(input_layer, x_decoded_mean)\n",
        "\n",
        "\n",
        "    reconstruction_loss = mse(K.flatten(input_layer), K.flatten(x_decoded_mean))\n",
        "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "\n",
        "    vae_loss = K.mean(reconstruction_loss + 0.1*kl_loss)\n",
        "    # vae_loss = K.mean(reconstruction_loss)\n",
        "    vae.add_loss(vae_loss)\n",
        "\n",
        "    return vae, z_mean, z_log_var\n",
        "\n",
        "# Example usage:\n",
        "input_dim = y_train.shape[1]  # Dimension of input and output\n",
        "latent_dim = 3   # Dimension of latent space\n",
        "intermediate_dim = 20  # Dimension of intermediate layer\n",
        "intermediate_dim_1 = 15\n",
        "intermediate_dim_2 = 10\n",
        "intermediate_dim_3 = 6\n",
        "# intermediate_dim_6 = 4\n",
        "\n",
        "vae, z_mean, z_log_var = create_vae(input_dim, latent_dim, intermediate_dim,intermediate_dim_1,intermediate_dim_2,intermediate_dim_3)\n",
        "vae.compile(optimizer='adam', metrics = ['R2Score'])\n",
        "# vae.compile(optimizer='adam', loss='mse', metrics = ['R2Score'])\n",
        "vae.summary()\n"
      ],
      "metadata": {
        "id": "k0h5LuZOIqzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae.fit(y_train, y_train, epochs=250, batch_size=128, validation_data=(y_val,y_val))\n",
        "vae.save('/content/drive/My Drive/SEISMIC MODELS/vae.h5')"
      ],
      "metadata": {
        "id": "QQdlsivdIrWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = tf.keras.models.load_model('/content/drive/My Drive/SEISMIC MODELS/vae.h5')"
      ],
      "metadata": {
        "id": "MtuJQ0twIuTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "y_pred_a = model_ann.predict(x_test)\n",
        "y_pred_c = [f\"log PSA {i}s\" for i in t]\n",
        "# y_pred_c = y_pred_a[:,[0,4,9,18]]\n",
        "y_pred_c.append(\"log PGA\");\n",
        "y_pred_c.append(\"log PGV\")\n",
        "y_pred = pd.DataFrame(y_pred_a, columns=y_pred_c)\n",
        "\n",
        "y_pred_a = vae.predict(y_pred)\n",
        "y_pred_c = [f\"log PSA {i}s\" for i in t]\n",
        "y_pred_c.append(\"log PGA\")\n",
        "y_pred_c.append(\"log PGV\")\n",
        "y_pred = pd.DataFrame(y_pred_a, columns=y_pred_c)"
      ],
      "metadata": {
        "id": "SFu0dw8CIxgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Calculate R^2 score for the first dataframe\n",
        "r2_score_1 = r2_score(y_test,y_pred)\n",
        "\n",
        "\n",
        "print(\"R^2 score for dataframe 1:\", r2_score_1)\n"
      ],
      "metadata": {
        "id": "ZdDIBiOAIzeV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}